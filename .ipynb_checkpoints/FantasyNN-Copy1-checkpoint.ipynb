{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Gradient Descent](https://gist.github.com/sagarmainkar/41d135a04d7d3bc4098f0664fe20cf3c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.preprocessing import MinMaxScaler, QuantileTransformer, StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = 100\n",
    "\n",
    "num_games = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler(x): \n",
    "    x = round(x,0)\n",
    "    return x\n",
    "#     if x <= 10: \n",
    "#         return 1\n",
    "#     elif x >10 and x <= 20: \n",
    "#         return 2 \n",
    "#     elif x > 20 and x <=30: \n",
    "#         return 3 \n",
    "#     elif x >30 and x <= 40: \n",
    "#         return 4\n",
    "#     else: \n",
    "#         return 5\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GameID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Date</th>\n",
       "      <th>Team</th>\n",
       "      <th>FDP</th>\n",
       "      <th>FDS</th>\n",
       "      <th>FD_change</th>\n",
       "      <th>FD_pos</th>\n",
       "      <th>OPP</th>\n",
       "      <th>Home</th>\n",
       "      <th>Away</th>\n",
       "      <th>MP</th>\n",
       "      <th>FG</th>\n",
       "      <th>FGA</th>\n",
       "      <th>FG%</th>\n",
       "      <th>3P</th>\n",
       "      <th>3PA</th>\n",
       "      <th>3P%</th>\n",
       "      <th>FT</th>\n",
       "      <th>FTA</th>\n",
       "      <th>FT%</th>\n",
       "      <th>ORB</th>\n",
       "      <th>DRB</th>\n",
       "      <th>TRB</th>\n",
       "      <th>AST</th>\n",
       "      <th>STL</th>\n",
       "      <th>BLK</th>\n",
       "      <th>TOV</th>\n",
       "      <th>PF</th>\n",
       "      <th>PTS</th>\n",
       "      <th>+-</th>\n",
       "      <th>4_MP</th>\n",
       "      <th>4_FG</th>\n",
       "      <th>4_FGA</th>\n",
       "      <th>4_FG%</th>\n",
       "      <th>4_3P</th>\n",
       "      <th>4_3PA</th>\n",
       "      <th>4_3P%</th>\n",
       "      <th>4_FT</th>\n",
       "      <th>4_FTA</th>\n",
       "      <th>4_FT%</th>\n",
       "      <th>4_ORB</th>\n",
       "      <th>4_DRB</th>\n",
       "      <th>4_TRB</th>\n",
       "      <th>4_AST</th>\n",
       "      <th>4_STL</th>\n",
       "      <th>4_BLK</th>\n",
       "      <th>4_TOV</th>\n",
       "      <th>4_PF</th>\n",
       "      <th>4_PTS</th>\n",
       "      <th>4_+-</th>\n",
       "      <th>4_FDP</th>\n",
       "      <th>4_FDS</th>\n",
       "      <th>NFDP</th>\n",
       "      <th>NN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>20191030-cha-sac</td>\n",
       "      <td>Malik Monk</td>\n",
       "      <td>20191030.0</td>\n",
       "      <td>cha</td>\n",
       "      <td>24.8</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>sac</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.26</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>11.0275</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.39575</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.25</td>\n",
       "      <td>-5.50</td>\n",
       "      <td>9.40</td>\n",
       "      <td>3900.0</td>\n",
       "      <td>14.6</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>20191030-cha-sac</td>\n",
       "      <td>Miles Bridges</td>\n",
       "      <td>20191030.0</td>\n",
       "      <td>cha</td>\n",
       "      <td>25.8</td>\n",
       "      <td>5700.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>sac</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.58</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.636</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>22.4850</td>\n",
       "      <td>3.25</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.35275</td>\n",
       "      <td>1.50</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0.36325</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.375</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2.75</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.50</td>\n",
       "      <td>9.50</td>\n",
       "      <td>-3.25</td>\n",
       "      <td>24.65</td>\n",
       "      <td>5950.0</td>\n",
       "      <td>12.1</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>20191030-cha-sac</td>\n",
       "      <td>Terry Rozier</td>\n",
       "      <td>20191030.0</td>\n",
       "      <td>cha</td>\n",
       "      <td>48.0</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>sac</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.53</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.467</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.714</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>20.1425</td>\n",
       "      <td>4.00</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.39775</td>\n",
       "      <td>1.50</td>\n",
       "      <td>4.25</td>\n",
       "      <td>0.18225</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.75</td>\n",
       "      <td>10.50</td>\n",
       "      <td>-4.75</td>\n",
       "      <td>25.45</td>\n",
       "      <td>6575.0</td>\n",
       "      <td>34.5</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                GameID           Name        Date Team   FDP     FDS  \\\n",
       "1197  20191030-cha-sac     Malik Monk  20191030.0  cha  24.8  4000.0   \n",
       "1198  20191030-cha-sac  Miles Bridges  20191030.0  cha  25.8  5700.0   \n",
       "1199  20191030-cha-sac   Terry Rozier  20191030.0  cha  48.0  7000.0   \n",
       "\n",
       "      FD_change  FD_pos  OPP  Home  Away     MP   FG   FGA    FG%   3P  3PA  \\\n",
       "1197      400.0     2.0  sac   1.0   0.0  20.26  7.0  12.0  0.583  4.0  7.0   \n",
       "1198      500.0     3.0  sac   1.0   0.0  33.58  7.0  11.0  0.636  3.0  5.0   \n",
       "1199      800.0     1.0  sac   1.0   0.0  34.53  7.0  15.0  0.467  3.0  6.0   \n",
       "\n",
       "        3P%   FT  FTA    FT%  ORB  DRB  TRB  AST  STL  BLK  TOV   PF   PTS  \\\n",
       "1197  0.571  0.0  0.0  0.000  0.0  4.0  4.0  2.0  0.0  0.0  1.0  2.0  18.0   \n",
       "1198  0.600  0.0  0.0  0.000  1.0  3.0  4.0  2.0  1.0  1.0  5.0  2.0  17.0   \n",
       "1199  0.500  5.0  7.0  0.714  2.0  3.0  5.0  6.0  3.0  2.0  4.0  4.0  22.0   \n",
       "\n",
       "        +-     4_MP  4_FG  4_FGA    4_FG%  4_3P  4_3PA    4_3P%  4_FT  4_FTA  \\\n",
       "1197  15.0  11.0275  2.00    4.5  0.39575  0.25   2.00  0.05000   0.0   0.00   \n",
       "1198  -1.0  22.4850  3.25    8.0  0.35275  1.50   3.75  0.36325   1.5   1.75   \n",
       "1199  -5.0  20.1425  4.00    9.0  0.39775  1.50   4.25  0.18225   1.0   1.00   \n",
       "\n",
       "      4_FT%  4_ORB  4_DRB  4_TRB  4_AST  4_STL  4_BLK  4_TOV  4_PF  4_PTS  \\\n",
       "1197  0.000   0.00   0.50   0.50   1.25   0.50   0.25   1.50  1.00   4.25   \n",
       "1198  0.375   1.25   2.75   4.00   2.00   0.25   0.25   1.75  1.50   9.50   \n",
       "1199  0.250   0.25   2.50   2.75   3.50   0.50   0.25   2.50  2.75  10.50   \n",
       "\n",
       "      4_+-  4_FDP   4_FDS  NFDP    NN  \n",
       "1197 -5.50   9.40  3900.0  14.6  25.0  \n",
       "1198 -3.25  24.65  5950.0  12.1  26.0  \n",
       "1199 -4.75  25.45  6575.0  34.5  48.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1920 = pd.read_csv(f'../NewData/AVG(19-20)[{num_games}].csv').drop('Unnamed: 0', axis =1)\n",
    "df_1819 = pd.read_csv(f'../NewData/AVG(18-19)[{num_games}].csv').drop('Unnamed: 0', axis =1)\n",
    "merged_df = df_1920.append(df_1819, ignore_index = True)\n",
    "merged_df.dropna(inplace = True)\n",
    "#merged_df = merged_df[merged_df.FDP != 0]\n",
    "merged_df['NN'] = merged_df.FDP.map(scaler)\n",
    "merged_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.8, 25.8, 48.0, ..., 0.0, 0.0, 0.0], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = merged_df.values[:,4]\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_col ='{}_MP 10_FG 10_FGA 10_FG% 10_3P 10_3PA 10_3P% 10_FT 10_FTA 10_FT% 10_ORB 10_DRB 10_TRB 10_AST 10_STL \\\n",
    "#10_BLK 10_TOV 10_PF 10_PTS 10_+- 10_FDP'.split()\n",
    "# col = ['MP', 'FG', 'FGA', 'FG%', '3P', '3PA',\n",
    "#                   '3P%', 'FT', 'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK',\n",
    "#                   'TOV', 'PF', 'PTS', '+-', 'FDP','FDS']\n",
    "col = ['MP', 'FG', 'FGA', '3P', '3PA',\n",
    "        'FT', 'FTA', 'TRB', 'AST', 'STL', 'BLK',\n",
    "        'PTS', '+-', 'FDP']#, 'FDS']\n",
    "x_col = [f'{num_games}_{i}' for i in col] + ['FDS', 'FD_change']\n",
    "X = np.array(merged_df[[i for i in x_col]])\n",
    "y = np.array(merged_df[['NN']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import losses\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras import optimizers\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "def sequential_AAN(x,y, metrics = ['accuracy']): \n",
    "    adams = Adam(learning_rate = .001)\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(x.shape[1], kernel_initializer = 'normal',input_dim = x.shape[1], activation = 'relu'))\n",
    "    \n",
    "    model.add(Dense(256, kernel_initializer = 'normal', activation = 'relu'))\n",
    "    \n",
    "    model.add(Dense(256, kernel_initializer = 'normal', activation = 'relu'))\n",
    "    \n",
    "    \n",
    "    model.add(Dense(1, kernel_initializer = 'normal', activation = 'linear'))#, activation = 'softmax'))\n",
    "    model.compile(loss = losses.mean_absolute_error, optimizer = adams, metrics = metrics)\n",
    "    return model\n",
    "\n",
    "def multivar_RNN(x,y): \n",
    "    model = Sequential()\n",
    "    units = 128\n",
    "    drop = .2\n",
    "    \n",
    "    #input\n",
    "    model.add(LSTM(units, return_sequences = True, input_shape = (1,x.shape[2])))\n",
    "    model.add(Dropout(drop))\n",
    "    \n",
    "    #1st\n",
    "    model.add(LSTM(units = units, return_sequences = True, input_shape = (1,x.shape[2])))\n",
    "    model.add(Dropout(drop))\n",
    "    \n",
    "    #2nd\n",
    "    model.add(LSTM(units = units, return_sequences = True, input_shape = (1,x.shape[2])))\n",
    "    model.add(Dropout(drop))\n",
    "    \n",
    "    #3rd\n",
    "    model.add(LSTM(units, return_sequences = False, input_shape = (1,x.shape[2])))\n",
    "    model.add(Dropout(drop))\n",
    "    \n",
    "    #output\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(optimizer = 'adam', loss = 'mae', metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1881/1881 [==============================] - 19s 10ms/step - loss: 8.3729 - accuracy: 0.0508 - val_loss: 7.5907 - val_accuracy: 0.0822\n",
      "Epoch 2/100\n",
      "1881/1881 [==============================] - 18s 9ms/step - loss: 7.6908 - accuracy: 0.0555 - val_loss: 7.5358 - val_accuracy: 0.0653\n",
      "Epoch 3/100\n",
      "1881/1881 [==============================] - 17s 9ms/step - loss: 7.6386 - accuracy: 0.0658 - val_loss: 7.7436 - val_accuracy: 0.0898\n",
      "Epoch 4/100\n",
      "1881/1881 [==============================] - 16s 9ms/step - loss: 7.6026 - accuracy: 0.0766 - val_loss: 7.4854 - val_accuracy: 0.0967\n",
      "Epoch 5/100\n",
      "1881/1881 [==============================] - 17s 9ms/step - loss: 7.5853 - accuracy: 0.0778 - val_loss: 7.4252 - val_accuracy: 0.0920\n",
      "Epoch 6/100\n",
      "1881/1881 [==============================] - 18s 9ms/step - loss: 7.5747 - accuracy: 0.0786 - val_loss: 7.4615 - val_accuracy: 0.1006\n",
      "Epoch 7/100\n",
      "1881/1881 [==============================] - 17s 9ms/step - loss: 7.5592 - accuracy: 0.0853 - val_loss: 7.4262 - val_accuracy: 0.0930\n",
      "Epoch 8/100\n",
      "1881/1881 [==============================] - 17s 9ms/step - loss: 7.5481 - accuracy: 0.0934 - val_loss: 7.4290 - val_accuracy: 0.0963\n",
      "Epoch 9/100\n",
      "1881/1881 [==============================] - 17s 9ms/step - loss: 7.5371 - accuracy: 0.0993 - val_loss: 7.4380 - val_accuracy: 0.0990\n",
      "Epoch 10/100\n",
      "1881/1881 [==============================] - 17s 9ms/step - loss: 7.5379 - accuracy: 0.1007 - val_loss: 7.4485 - val_accuracy: 0.1096\n",
      "Epoch 11/100\n",
      "1881/1881 [==============================] - 16s 9ms/step - loss: 7.5242 - accuracy: 0.1015 - val_loss: 7.5118 - val_accuracy: 0.1180\n",
      "Epoch 12/100\n",
      "1881/1881 [==============================] - 17s 9ms/step - loss: 7.5218 - accuracy: 0.1024 - val_loss: 7.4587 - val_accuracy: 0.1133\n",
      "Epoch 13/100\n",
      "1881/1881 [==============================] - 18s 10ms/step - loss: 7.5144 - accuracy: 0.1043 - val_loss: 7.6026 - val_accuracy: 0.1150\n",
      "Epoch 14/100\n",
      "1881/1881 [==============================] - 18s 9ms/step - loss: 7.5005 - accuracy: 0.1064 - val_loss: 7.4187 - val_accuracy: 0.1086\n",
      "Epoch 15/100\n",
      "1881/1881 [==============================] - 18s 10ms/step - loss: 7.5071 - accuracy: 0.1059 - val_loss: 7.4528 - val_accuracy: 0.1151\n",
      "Epoch 16/100\n",
      "1881/1881 [==============================] - 18s 9ms/step - loss: 7.4757 - accuracy: 0.1077 - val_loss: 7.4290 - val_accuracy: 0.1097\n",
      "Epoch 17/100\n",
      "1881/1881 [==============================] - 17s 9ms/step - loss: 7.4865 - accuracy: 0.1065 - val_loss: 7.4642 - val_accuracy: 0.1184\n",
      "Epoch 18/100\n",
      "1881/1881 [==============================] - 17s 9ms/step - loss: 7.4680 - accuracy: 0.1064 - val_loss: 7.4654 - val_accuracy: 0.1094\n",
      "Epoch 19/100\n",
      "1881/1881 [==============================] - 17s 9ms/step - loss: 7.4743 - accuracy: 0.1056 - val_loss: 7.4392 - val_accuracy: 0.1044\n",
      "Epoch 20/100\n",
      "1881/1881 [==============================] - 17s 9ms/step - loss: 7.4620 - accuracy: 0.1080 - val_loss: 7.4933 - val_accuracy: 0.1074\n",
      "Epoch 21/100\n",
      "1881/1881 [==============================] - 17s 9ms/step - loss: 7.4649 - accuracy: 0.1087 - val_loss: 7.4566 - val_accuracy: 0.0889\n",
      "Epoch 22/100\n",
      "1881/1881 [==============================] - 17s 9ms/step - loss: 7.4523 - accuracy: 0.1078 - val_loss: 7.4151 - val_accuracy: 0.1016\n",
      "Epoch 23/100\n",
      "1881/1881 [==============================] - 17s 9ms/step - loss: 7.4332 - accuracy: 0.1081 - val_loss: 7.4616 - val_accuracy: 0.0858\n",
      "Epoch 24/100\n",
      "1881/1881 [==============================] - 16s 9ms/step - loss: 7.4286 - accuracy: 0.1087 - val_loss: 7.4661 - val_accuracy: 0.0993\n",
      "Epoch 25/100\n",
      "1881/1881 [==============================] - 16s 9ms/step - loss: 7.4252 - accuracy: 0.1091 - val_loss: 7.5008 - val_accuracy: 0.1178\n",
      "Epoch 26/100\n",
      "1881/1881 [==============================] - 18s 10ms/step - loss: 7.4204 - accuracy: 0.1100 - val_loss: 7.5823 - val_accuracy: 0.1185\n",
      "Epoch 27/100\n",
      "1881/1881 [==============================] - 18s 10ms/step - loss: 7.4188 - accuracy: 0.1086 - val_loss: 7.4431 - val_accuracy: 0.1060\n",
      "Epoch 28/100\n",
      "1881/1881 [==============================] - 18s 9ms/step - loss: 7.4169 - accuracy: 0.1084 - val_loss: 7.4641 - val_accuracy: 0.1122\n",
      "Epoch 29/100\n",
      "1881/1881 [==============================] - 17s 9ms/step - loss: 7.4027 - accuracy: 0.1087 - val_loss: 7.4666 - val_accuracy: 0.1148\n",
      "Epoch 30/100\n",
      "1881/1881 [==============================] - 17s 9ms/step - loss: 7.3974 - accuracy: 0.1109 - val_loss: 7.4582 - val_accuracy: 0.1083\n",
      "Epoch 31/100\n",
      "1881/1881 [==============================] - 16s 9ms/step - loss: 7.3913 - accuracy: 0.1102 - val_loss: 7.4607 - val_accuracy: 0.0955\n",
      "Epoch 32/100\n",
      "1881/1881 [==============================] - 16s 9ms/step - loss: 7.3777 - accuracy: 0.1110 - val_loss: 7.4835 - val_accuracy: 0.1149\n",
      "Epoch 33/100\n",
      "1881/1881 [==============================] - 16s 9ms/step - loss: 7.3669 - accuracy: 0.1109 - val_loss: 7.4605 - val_accuracy: 0.1061\n",
      "Epoch 34/100\n",
      "1881/1881 [==============================] - 16s 9ms/step - loss: 7.3847 - accuracy: 0.1091 - val_loss: 7.4661 - val_accuracy: 0.1090\n",
      "Epoch 35/100\n",
      "1881/1881 [==============================] - 17s 9ms/step - loss: 7.3673 - accuracy: 0.1111 - val_loss: 7.4893 - val_accuracy: 0.1170\n",
      "Epoch 36/100\n",
      "1881/1881 [==============================] - 18s 9ms/step - loss: 7.3505 - accuracy: 0.1114 - val_loss: 7.4823 - val_accuracy: 0.1027\n",
      "Epoch 37/100\n",
      "1881/1881 [==============================] - 17s 9ms/step - loss: 7.3510 - accuracy: 0.1103 - val_loss: 7.5041 - val_accuracy: 0.1166\n",
      "Epoch 38/100\n",
      "1881/1881 [==============================] - 16s 9ms/step - loss: 7.3471 - accuracy: 0.1117 - val_loss: 7.4924 - val_accuracy: 0.1114\n",
      "Epoch 39/100\n",
      "1881/1881 [==============================] - 16s 9ms/step - loss: 7.3233 - accuracy: 0.1097 - val_loss: 7.4903 - val_accuracy: 0.1021\n",
      "Epoch 40/100\n",
      "1881/1881 [==============================] - 17s 9ms/step - loss: 7.3266 - accuracy: 0.1084 - val_loss: 7.5157 - val_accuracy: 0.0880\n",
      "Epoch 41/100\n",
      "1881/1881 [==============================] - 16s 9ms/step - loss: 7.3239 - accuracy: 0.1104 - val_loss: 7.4980 - val_accuracy: 0.1121\n",
      "Epoch 42/100\n",
      "1881/1881 [==============================] - 16s 9ms/step - loss: 7.3226 - accuracy: 0.1102 - val_loss: 7.5227 - val_accuracy: 0.1115\n",
      "Epoch 43/100\n",
      "1881/1881 [==============================] - 18s 9ms/step - loss: 7.3006 - accuracy: 0.1112 - val_loss: 7.5241 - val_accuracy: 0.1162\n",
      "Epoch 44/100\n",
      "1881/1881 [==============================] - 18s 10ms/step - loss: 7.2953 - accuracy: 0.1119 - val_loss: 7.5417 - val_accuracy: 0.1091\n",
      "Epoch 45/100\n",
      "1881/1881 [==============================] - 18s 9ms/step - loss: 7.2916 - accuracy: 0.1138 - val_loss: 7.5154 - val_accuracy: 0.1094\n",
      "Epoch 46/100\n",
      "1881/1881 [==============================] - 17s 9ms/step - loss: 7.2878 - accuracy: 0.1099 - val_loss: 7.5171 - val_accuracy: 0.1141\n",
      "Epoch 47/100\n",
      "1881/1881 [==============================] - 18s 9ms/step - loss: 7.2526 - accuracy: 0.1125 - val_loss: 7.5297 - val_accuracy: 0.1107\n",
      "Epoch 48/100\n",
      "1881/1881 [==============================] - 17s 9ms/step - loss: 7.2426 - accuracy: 0.1124 - val_loss: 7.5621 - val_accuracy: 0.1129\n",
      "Epoch 49/100\n",
      "1881/1881 [==============================] - 18s 10ms/step - loss: 7.2451 - accuracy: 0.1120 - val_loss: 7.5437 - val_accuracy: 0.1163\n",
      "Epoch 50/100\n",
      "1881/1881 [==============================] - 18s 9ms/step - loss: 7.2302 - accuracy: 0.1132 - val_loss: 7.5592 - val_accuracy: 0.1166\n",
      "Epoch 51/100\n",
      "1881/1881 [==============================] - 18s 10ms/step - loss: 7.2152 - accuracy: 0.1135 - val_loss: 7.5264 - val_accuracy: 0.1150\n",
      "Epoch 52/100\n",
      "1881/1881 [==============================] - 17s 9ms/step - loss: 7.2212 - accuracy: 0.1135 - val_loss: 7.5518 - val_accuracy: 0.1161\n",
      "Epoch 53/100\n",
      "1881/1881 [==============================] - 18s 9ms/step - loss: 7.2127 - accuracy: 0.1128 - val_loss: 7.5508 - val_accuracy: 0.1140\n",
      "Epoch 54/100\n",
      "1881/1881 [==============================] - 19s 10ms/step - loss: 7.1856 - accuracy: 0.1141 - val_loss: 7.6106 - val_accuracy: 0.1140\n",
      "Epoch 55/100\n",
      "1881/1881 [==============================] - 18s 9ms/step - loss: 7.1745 - accuracy: 0.1116 - val_loss: 7.5957 - val_accuracy: 0.1128\n",
      "Epoch 56/100\n",
      "1881/1881 [==============================] - 18s 9ms/step - loss: 7.1792 - accuracy: 0.1112 - val_loss: 7.6052 - val_accuracy: 0.1144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "1881/1881 [==============================] - 18s 10ms/step - loss: 7.1480 - accuracy: 0.1130 - val_loss: 7.6118 - val_accuracy: 0.1160\n",
      "Epoch 58/100\n",
      "1881/1881 [==============================] - 18s 9ms/step - loss: 7.1303 - accuracy: 0.1141 - val_loss: 7.5935 - val_accuracy: 0.1174\n",
      "Epoch 59/100\n",
      "1881/1881 [==============================] - 17s 9ms/step - loss: 7.1287 - accuracy: 0.1137 - val_loss: 7.6137 - val_accuracy: 0.1114\n",
      "Epoch 60/100\n",
      "1881/1881 [==============================] - 17s 9ms/step - loss: 7.1267 - accuracy: 0.1123 - val_loss: 7.6028 - val_accuracy: 0.1142\n",
      "Epoch 61/100\n",
      "1881/1881 [==============================] - 17s 9ms/step - loss: 7.1128 - accuracy: 0.1145 - val_loss: 7.6223 - val_accuracy: 0.1165\n",
      "Epoch 62/100\n",
      "1881/1881 [==============================] - 17s 9ms/step - loss: 7.0915 - accuracy: 0.1137 - val_loss: 7.5900 - val_accuracy: 0.1179\n",
      "Epoch 63/100\n",
      "1881/1881 [==============================] - 17s 9ms/step - loss: 7.0845 - accuracy: 0.1135 - val_loss: 7.5624 - val_accuracy: 0.1144\n",
      "Epoch 64/100\n",
      "1881/1881 [==============================] - 17s 9ms/step - loss: 7.0789 - accuracy: 0.1137 - val_loss: 7.6235 - val_accuracy: 0.1122\n",
      "Epoch 65/100\n",
      "1881/1881 [==============================] - 17s 9ms/step - loss: 7.0519 - accuracy: 0.1137 - val_loss: 7.6304 - val_accuracy: 0.1200\n",
      "Epoch 66/100\n",
      "1881/1881 [==============================] - 17s 9ms/step - loss: 7.0390 - accuracy: 0.1143 - val_loss: 7.6509 - val_accuracy: 0.1158\n",
      "Epoch 67/100\n",
      "1881/1881 [==============================] - 17s 9ms/step - loss: 7.0439 - accuracy: 0.1108 - val_loss: 7.6139 - val_accuracy: 0.1158\n",
      "Epoch 68/100\n",
      "1881/1881 [==============================] - 17s 9ms/step - loss: 7.0348 - accuracy: 0.1161 - val_loss: 7.6611 - val_accuracy: 0.1156\n",
      "Epoch 69/100\n",
      "1881/1881 [==============================] - 17s 9ms/step - loss: 7.0176 - accuracy: 0.1135 - val_loss: 7.6829 - val_accuracy: 0.1180\n",
      "Epoch 70/100\n",
      "1881/1881 [==============================] - 17s 9ms/step - loss: 6.9939 - accuracy: 0.1151 - val_loss: 7.6535 - val_accuracy: 0.1167\n",
      "Epoch 71/100\n",
      "1881/1881 [==============================] - 17s 9ms/step - loss: 6.9945 - accuracy: 0.1138 - val_loss: 7.6375 - val_accuracy: 0.1146\n",
      "Epoch 72/100\n",
      "1881/1881 [==============================] - 16s 8ms/step - loss: 6.9492 - accuracy: 0.1144 - val_loss: 7.6482 - val_accuracy: 0.1151\n",
      "Epoch 73/100\n",
      "1881/1881 [==============================] - 16s 8ms/step - loss: 6.9853 - accuracy: 0.1130 - val_loss: 7.6454 - val_accuracy: 0.1161\n",
      "Epoch 74/100\n",
      "1881/1881 [==============================] - 16s 8ms/step - loss: 6.9579 - accuracy: 0.1145 - val_loss: 7.6679 - val_accuracy: 0.1135\n",
      "Epoch 75/100\n",
      "1881/1881 [==============================] - 16s 8ms/step - loss: 6.9209 - accuracy: 0.1146 - val_loss: 7.7178 - val_accuracy: 0.1153\n",
      "Epoch 76/100\n",
      "1881/1881 [==============================] - 16s 8ms/step - loss: 6.9249 - accuracy: 0.1151 - val_loss: 7.6328 - val_accuracy: 0.1132\n",
      "Epoch 77/100\n",
      "1881/1881 [==============================] - 16s 8ms/step - loss: 6.9140 - accuracy: 0.1147 - val_loss: 7.6775 - val_accuracy: 0.1074\n",
      "Epoch 78/100\n",
      "1881/1881 [==============================] - 16s 8ms/step - loss: 6.8978 - accuracy: 0.1153 - val_loss: 7.6718 - val_accuracy: 0.1112\n",
      "Epoch 79/100\n",
      "1881/1881 [==============================] - 16s 9ms/step - loss: 6.9070 - accuracy: 0.1143 - val_loss: 7.6768 - val_accuracy: 0.1135\n",
      "Epoch 80/100\n",
      "1881/1881 [==============================] - 16s 8ms/step - loss: 6.8851 - accuracy: 0.1153 - val_loss: 7.6903 - val_accuracy: 0.1163\n",
      "Epoch 81/100\n",
      "1881/1881 [==============================] - 16s 8ms/step - loss: 6.8734 - accuracy: 0.1157 - val_loss: 7.6919 - val_accuracy: 0.1161\n",
      "Epoch 82/100\n",
      "1881/1881 [==============================] - 17s 9ms/step - loss: 6.8518 - accuracy: 0.1128 - val_loss: 7.6831 - val_accuracy: 0.1148\n",
      "Epoch 83/100\n",
      "1881/1881 [==============================] - 17s 9ms/step - loss: 6.8527 - accuracy: 0.1150 - val_loss: 7.6933 - val_accuracy: 0.1143\n",
      "Epoch 84/100\n",
      "1879/1881 [============================>.] - ETA: 0s - loss: 6.8439 - accuracy: 0.1139"
     ]
    }
   ],
   "source": [
    "epo = 100\n",
    "batches = 16\n",
    "type_nn = 'mult_rnn'\n",
    "\n",
    "train_size = .7 \n",
    "test_size = .3 \n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, train_size = train_size)\n",
    "min_max = MinMaxScaler(feature_range = (0,1)) \n",
    "standard = StandardScaler()\n",
    "\n",
    "if type_nn == 'seq_an':\n",
    "    model = sequential_AAN(X, y)\n",
    "    model.fit(x_train, y_train, epochs = epo, batch_size=batches, validation_data = (x_test, y_test))\n",
    "    \n",
    "elif type_nn == 'mult_rnn':\n",
    "    lookback = 1\n",
    "    x_train = standard.fit_transform(x_train)\n",
    "    x_test = standard.fit_transform(x_test)\n",
    "    x_test = x_test.reshape((x_test.shape[0], lookback, x_test.shape[1]))\n",
    "    x_train = x_train.reshape((x_train.shape[0],lookback, x_train.shape[1]))\n",
    "    #y_train = y_train.reshape((1, y_train.shape[0], y_train.shape[1]))\n",
    "\n",
    "\n",
    "    model = multivar_RNN(x_train, y_train)\n",
    "    history = model.fit(x_train, y_train, epochs = epo, batch_size=batches, validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test).flatten()\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.scatter(predictions, y_test, s = 10)\n",
    "plt.xlabel('Predictions')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlim(20,80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'../models/{type_nn}_Model_{num_games}[{epo}_{batches}].h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,y, train_size = train_size)\n",
    "\n",
    "reg = LinearRegression()\n",
    "x_train = standard.fit_transform(x_train)\n",
    "x_test = standard.fit_transform(x_test)\n",
    "reg.fit(x_train, y_train)\n",
    "coef = list(zip(x_col, [round(i,3) for i in reg.coef_[0]]))\n",
    "print(coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = reg.predict(x_test)\n",
    "# plt.figure(figsize =(8,5))\n",
    "# plt.scatter(y_test, predictions)\n",
    "# plt.xlabel('Actual'); plt.ylabel('Predictions'); plt.title('All Coeficients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  cal_cost(theta,X,y):\n",
    "    '''\n",
    "    \n",
    "    Calculates the cost for given X and Y. The following shows and example of a single dimensional X\n",
    "    theta = Vector of thetas \n",
    "    X     = Row of X's np.zeros((2,j))\n",
    "    y     = Actual y's np.zeros((2,1))\n",
    "    \n",
    "    where:\n",
    "        j is the no of features\n",
    "    '''\n",
    "    \n",
    "    m = len(y)\n",
    "    \n",
    "    predictions = X.dot(theta)\n",
    "    cost = (1/2*m) * np.sum(np.square(predictions-y))\n",
    "    return cost\n",
    "\n",
    "def gradient_descent(X,y,theta,learning_rate=0.01,iterations=100):\n",
    "    '''\n",
    "    X    = Matrix of X with added bias units\n",
    "    y    = Vector of Y\n",
    "    theta=Vector of thetas np.random.randn(j,1)\n",
    "    learning_rate \n",
    "    iterations = no of iterations\n",
    "    \n",
    "    Returns the final theta vector and array of cost history over no of iterations\n",
    "    '''\n",
    "    #print(X)\n",
    "    #print(y)\n",
    "    m = len(y)\n",
    "    cost_history = np.zeros(iterations)\n",
    "    theta_history = np.zeros((iterations,2))\n",
    "    for it in range(iterations):\n",
    "        \n",
    "        prediction = np.dot(X,theta)\n",
    "        \n",
    "        theta = theta -(1/m)*learning_rate*( X.T.dot((prediction - y)))\n",
    "        theta_history[it,:] =theta.T\n",
    "        cost_history[it]  = cal_cost(theta,X,y)\n",
    "        \n",
    "    return theta, cost_history, theta_history\n",
    "\n",
    "# def stochastic_descent(X,y, learning_rate = .0001, iterations = 1): \n",
    "#     theta = None\n",
    "#     m = len(y)\n",
    "#     cost_history = np.zeros(iterations)\n",
    "#     for I in range(iterations):\n",
    "#         cost = 0.0\n",
    "#         for i in range(m):\n",
    "#             rand_ind = np.random.randint(0,m)\n",
    "#             X_i = X[rand_ind, :].reshape(1, X.shape[1])\n",
    "#             y_i = y[rand_ind].reshape(1,1)\n",
    "#             prediction = np.dot(X_i, theta)\n",
    "#             theta = theta - (1/m) * learning_rate * (X_i.T.dot((prediction - y_i)))\n",
    "#             cost += cal_cost(theta, X_i, y_i)\n",
    "#         cost_history = cost \n",
    "#     return theta, cost_history\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = .001\n",
    "n_iter = 1000 \n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "X_b = np.c_[np.ones((len(X),1)),X]\n",
    "X.reshape(-1,1,1)\n",
    "#theta, cost_history, theta_history = gradient_descent(X,y,theta, lr, n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(new_x,new_y, train_size = train_size)\n",
    "\n",
    "reg2 = LinearRegression()\n",
    "x_train2 = standard.fit_transform(x_train2)\n",
    "x_test2 = standard.fit_transform(x_test2)\n",
    "reg2.fit(x_train2, y_train2)\n",
    "coef2 = list(zip(new_coef, [round(i,3) for i in reg2.coef_[0]]))\n",
    "print(coef2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = reg2.predict(x_test2)\n",
    "plt.figure(figsize =(12,8))\n",
    "plt.scatter(y_test2, predictions2)\n",
    "plt.xlabel('Actual'); plt.ylabel('Predictions'); plt.title('New Coeficients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
